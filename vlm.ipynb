{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "0.20.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated random target pos in 1 iterations\n",
      "Reward : 0.38638389110565186\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAALIElEQVR4nO3dvXLdxgEFYDCjNhOyTSPlITypU1CdH4F6OPkR0slF0ucl7CcgXSVFZm4KxScSDAU/XAC7i+/raIMXi/vDM7tHe3F3u91uAwAMw/C7swcAQD2EAgAhFAAIoQBACAUAQigAEEIBgBAKAIRQACCEAgAhFAAIoQBAvFl64MvLy1c///Ufv/zmmO+/+8Orjxn//1LHGO/yY1q7pqmxzI1t6+O29jxccbxLjmntmkqN9/7+/jfHjJkpABBCAYAQCgCEUAAgFhfNY1sLlRLnWlKolDjP1LmOuqa9znPkuc58P8z9ztLfm7O1ACxxrrPOU+pcNT13pc5V+zU9Pd7PPq6ZAgAhFAAIoQBALO4Utq7bbnmcLebW17auIc5d01HXMzWWHq5prNR7qFR/Mfe4U2Mptblu7tylzF3T1g5v7pr2up6px279mpZ81kt1rWYKAIRQACCEAgAhFACIu9vtdlty4A8//jx7zJZSY6+CzeOe/7hbzuNxPe5rH3fJY1/1cX1LKgCrCAUAQigAEIs7hfGd15bYa53xzHPVfJ6jz3XWuWs+z5Hnqvk8R57LNS0/z9Pj29ljzBQACKEAQAgFAGLXTmHKkWu7rz3vmecu9cVarfUFNZ37yE6hxLlbf76de/9z6xQAWEUoABBCAYAQCgDE6V+IN3ZmcTPlzMJybK87otU0PmP5tt4+XzWNZRiu8Z7xhXgArCIUAAihAEAU3bxW05rcWG3rl2NX6Aum1LQ2PXaF524Y6h5fzWMbhvbGZ/MaAKsIBQBCKAAQQgGAqK5onnLVcndOa+M33s+M97O9xjsM7Y35qPEqmgFYRSgAEEIBgCj6hXhLtLbWN9Z6XzAM7V+D8S/X+jW0Pv5hqOsafCEeAKsIBQBCKAAQRfcplHBUNzAMx64rjvXWgQxDf9fU43vRNb1O69dknwIAqwgFAEIoABBCAYBYXDTf3d3tPZZdffz002Hnar2MWqLHAnvsCtc4DN6vJdV+jTavAbCKUAAghAIAcZlOYcrz8/Pq36l9zXAvV7zuK17zMLjuMxx13ToFAFYRCgCEUAAghAIAcemieYmjNr19eP9u9pgtxfgWVyjclvA8fOZ5+J/WN1IqmgFYRSgAEEIBgNApXNC4J1myXvnw8LDTaOp1VIez1Jlr+2M1rfV7Xr5t/Ny48xoAqwgFAEIoABA6BVjhzJ6hprXzsdrX0mty5nNlnwIAqwgFAEIoABBCAYB4c/YAoCU1FZi1lbtjNT1XY1d97p4e72ePMVMAIIQCACEUAIgVncLfRj//pehAgL6M1+1r6himxlJTz7BkLHs9n2YKAIRQACCEAgAhFACIFd+S+vfRf1E0cz3ju9bVpKaidImaiuelWn+O3XkNgFWEAgAhFACIxZvXPn7601c/f//dcXegenh4WP074ztklVq//PD+XZHHOdOWu4dteQ16VPOacu1r9P8afXb+PHHMH0c/vznxTndL1P6cb2GmAEAIBQBCKAAQi/cpvLy87DyU1+ltba/mteulznxN9up+tvQxZ6rpc/HP0Wvy+4lj/j3aB+JzUJZ9CgCsIhQACKEAQAgFAKLJormm4qYUhdq+ShXPiubztf5ZOfM1UTQDsIpQACCEAgCx+AvxzmRdFLYZv896+CyNr6G1z9KS8Z75OpkpABBCAYAQCgCEUAAgFm9e++HHn3cZQGsl0RY9lHtjPbxuW+4m93H0LZ6t6eF1W6K3z1yp1+3+/n72GDMFAEIoABBCAYDYdfPaFdYve1u7pG9T79ceP6e9bdpbMv5Sr6OZAgAhFAAIoQBACAUAomjR3GNhxWdeW1rWW/E8pdQ/IjBTACCEAgAhFACIxV+I9/LysvNQ6tTj2uMWPXYKV/xCvCV6fK2XuMJn/enx7ewxZgoAhFAAIIQCALHrF+LRpquuKXNtV9jLsISZAgAhFAAIoQBACAUAQtH8hasWS/Cl8efgqv/wYOq6r/A3wkwBgBAKAIRQACAu0ylcYS1wq6uuGcNaV9jgZqYAQAgFAEIoABBCAYBYXDS3XqgoU7+t9df2SN5Hn3nPTOvh/WGmAEAIBQBCKAAQ3Wxe62Et7yjWg2EfU5+t1v42mSkAEEIBgBAKAEQ3nQJwnCt8MVwprd20yEwBgBAKAIRQACCEAgDRZNFce1FTk6sWgB/ev9vlcR8eHmaPeX5+3uXc9KH2DW5mCgCEUAAghAIA0USnUNN6G+fbqy8oZa536LFzmPqMXrXP2qKmDW5mCgCEUAAghAIAIRQAiNOLZiVyWco9aN+ZxbOZAgAhFAAIoQBAHN4p6BDK0R/0Y8kX7R2l1OY6d2cr58gv0TNTACCEAgAhFACI0/cpwNXU1B/Qrr32MpgpABBCAYAQCgCEUAAgdi2abVSjhNrvtNabcRFuM1sbSm1wM1MAIIQCACEUAIiinYIOYV/WYIE1xn8znh7vZ3/HTAGAEAoAhFAAIIQCAFG0aFaEljNV2ivyOcORn2vv8fOZKQAQQgGAEAoAhFAAIIQCACEUAAihAEDsepMdgDXGeyLsWziemQIAIRQACKEAQAgFAELRXAmF2rc9Pz9/9fPDw8NJI7mGj59+OnsInMhMAYAQCgCEUAAgdAon0SFs11rHYI1+u6kb/Pjs7MtMAYAQCgCEUAAghAIAoWimeePieRjOLZ8Vy7TMTAGAEAoAhFAAIHQKdGmqZ1hrauMU53N3tn2ZKQAQQgGAEAoAhE7hANY8gVaYKQAQQgGAEAoAhFAAIBTNQNPcna0sMwUAQigAEEIBgNAp7MB6JtAqMwUAQigAEEIBgBAKAISi+ZWUyv1wp7V+uDvbdmYKAIRQACCEAgChU4D/Gq876xi4IjMFAEIoABBCAYDQKQDds29hOTMFAEIoABBCAYAQCgCEonklBRXQMzMFAEIoABBCAYDQKQCXM/Vlh/rCz8wUAAihAEAIBQBCKAAQiuYZyifgSswUAAihAEAIBQBCpwAwuDvbr8wUAAihAEAIBQBCp/CFq64hAvzKTAGAEAoAhFAAIIQCAHHpolmxDHzLVe/OZqYAQAgFAEIoABBCAYAQCgCEUAAghAIAIRQAiEtvXgNY4wp3ZzNTACCEAgAhFACIy3QKPa79UdbUF6DB1ZgpABBCAYAQCgDEZToFmDPunXQMzOlx34KZAgAhFAAIoQBACAUA4u52u92WHPjy8rLzUMpSEvJaPZSGHKv2vztPj29njzFTACCEAgAhFAAIm9cACpnqoWrvGcbMFAAIoQBACAUAQigAEN0Uza2VOQA1MlMAIIQCACEUAIhuOgWAGrV2Rz8zBQBCKAAQQgGAEAoARJNFc+1FDUCrzBQACKEAQAgFAKLJTgGgVbXfnc1MAYAQCgCEUAAgmugUalpvA+iZmQIAIRQACKEAQAgFAKKJohmgZzXdnc1MAYAQCgCEUAAgqusUbFQDOI+ZAgAhFAAIoQBACAUAorqiGeDqztzMZqYAQAgFAEIoABCndwo2qwHUw0wBgBAKAIRQACBO7xQA+P/G+xaGYb8+1kwBgBAKAIRQACCEAgBxeNFssxpAvcwUAAihAEAIBQDC5jWABu11Ix4zBQBCKAAQQgGAEAoAxK5Fs41qAG0xUwAghAIAIRQACJvXADpQ6u5sZgoAhFAAIIQCAFG0U7AvAaBtZgoAhFAAIIQCACEUAAib1wA6NbWhbY6ZAgAhFAAIoQBAbO4UbFQD6I+ZAgAhFAAIoQBACAUAQigAEEIBgBAKAIRQACDubrfb7exBAFAHMwUAQigAEEIBgBAKAIRQACCEAgAhFAAIoQBACAUA4j85FTGPehN25QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rx150_env \n",
    "importlib.reload(rx150_env)\n",
    "\n",
    "from rx150_env import RX150Env \n",
    "\n",
    "urdf_path = \"/interbotix_ros_manipulators/interbotix_ros_xsarms/interbotix_xsarm_descriptions/urdf/rx150.urdf\"\n",
    "\n",
    "# print(urdf_path)\n",
    "\n",
    "rx_env = RX150Env(urdf_path,headless=True,image_height=84,image_width=84)\n",
    "\n",
    "rx_env.reset()\n",
    "rx_env.step(np.zeros(6))\n",
    "rx_img = rx_env.render()\n",
    "\n",
    "rw,_ = rx_env.get_reward_and_terminal()\n",
    "print(f\"Reward : {rw}\")\n",
    "\n",
    "rx_env.close()\n",
    "\n",
    "plt.imshow(rx_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3104],\n",
      "        [0.3134]])\n",
      "tensor([[31.0354, 31.3393]], grad_fn=<TBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "inputs = processor(text=[\"A 3D model of a robot arm and a red dot with a blue end-effector. The robot arm's blue end effector is touching the red dot\",\"A 3D model of a robot arm and a red dot with a blue end-effector.\"], images=rx_img, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1) \n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.get_text_features(**{k: inputs[k] for k in [\"input_ids\", \"attention_mask\"]})\n",
    "    image_features = model.get_image_features(**{k: inputs[k] for k in [\"pixel_values\"]})\n",
    "\n",
    "text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "similarity_score = (text_features @ image_features.T) \n",
    "\n",
    "print(similarity_score)\n",
    "\n",
    "print(logits_per_image)\n",
    "# print(probs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
