{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-f7e8gkm5\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-f7e8gkm5\n",
      "\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit fc8764c9a618add64c33e83720f974750bcd0978\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.50.0.dev0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.50.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.50.0.dev0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.50.0.dev0) (2025.1.31)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.50.0.dev0-py3-none-any.whl size=10938856 sha256=edd30da2b12fc33f5b1ac7658e9e31ab44368ae6ac027e0d39d0a01bc71f5058\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8kpch83v/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.49.0\n",
      "    Uninstalling transformers-4.49.0:\n",
      "      Successfully uninstalled transformers-4.49.0\n",
      "Successfully installed transformers-4.50.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RX150Env' object has no attribute 'clip_processor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 26\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# action = np.random.choice([-1,0,1], size=5)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# action = np.append(action,np.random.choice([0,1],size=1))\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# for _ in range(100):\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#     rx_env.step(action)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m rx_img \u001b[38;5;241m=\u001b[39m rx_env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m---> 26\u001b[0m rw,_ \u001b[38;5;241m=\u001b[39m \u001b[43mrx_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_reward_and_terminal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReward : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m rx_env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/IFT6163-Group4/rx150_env.py:251\u001b[0m, in \u001b[0;36mRX150Env.get_reward_and_terminal\u001b[0;34m(self, ob)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[1;32m    249\u001b[0m     rx_img \u001b[38;5;241m=\u001b[39m ob[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 251\u001b[0m image_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_processor\u001b[49m(images\u001b[38;5;241m=\u001b[39mrx_img, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    254\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_model\u001b[38;5;241m.\u001b[39mget_image_features(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: image_inputs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RX150Env' object has no attribute 'clip_processor'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rx150_env \n",
    "importlib.reload(rx150_env)\n",
    "\n",
    "from rx150_env import RX150Env \n",
    "\n",
    "urdf_path = \"/interbotix_ros_manipulators/interbotix_ros_xsarms/interbotix_xsarm_descriptions/urdf/rx150.urdf\"\n",
    "\n",
    "# print(urdf_path)\n",
    "\n",
    "rx_env = RX150Env(urdf_path,headless=True,image_height=84,image_width=84)\n",
    "\n",
    "rx_env.reset()\n",
    "\n",
    "# action = np.random.choice([-1,0,1], size=5)\n",
    "# action = np.append(action,np.random.choice([0,1],size=1))\n",
    "\n",
    "# for _ in range(100):\n",
    "#     rx_env.step(action)\n",
    "\n",
    "rx_img = rx_env.render()\n",
    "\n",
    "rw,_ = rx_env.get_reward_and_terminal()\n",
    "print(f\"Reward : {rw}\")\n",
    "\n",
    "rx_env.close()\n",
    "\n",
    "plt.imshow(rx_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126659ff304449f6883627d4e0168c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'> for this kind of AutoModel: AutoModel.\nModel type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, AriaConfig, AriaTextConfig, ASTConfig, AutoformerConfig, BambaConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChameleonConfig, ChineseCLIPConfig, ChineseCLIPVisionConfig, ClapConfig, CLIPConfig, CLIPTextConfig, CLIPVisionConfig, CLIPSegConfig, ClvpConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, DabDetrConfig, DacConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DbrxConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DepthProConfig, DetaConfig, DetrConfig, DiffLlamaConfig, DinatConfig, Dinov2Config, Dinov2WithRegistersConfig, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FalconMambaConfig, FastSpeech2ConformerConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GemmaConfig, Gemma2Config, GitConfig, GlmConfig, GLPNConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeSharedConfig, GraphormerConfig, GroundingDinoConfig, GroupViTConfig, HeliumConfig, HieraConfig, HubertConfig, IBertConfig, IdeficsConfig, Idefics2Config, Idefics3Config, Idefics3VisionConfig, IJepaConfig, ImageGPTConfig, InformerConfig, JambaConfig, JetMoeConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MambaConfig, Mamba2Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MimiConfig, MistralConfig, MixtralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, ModernBertConfig, MoonshineConfig, MoshiConfig, MPNetConfig, MptConfig, MraConfig, MT5Config, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NatConfig, NemotronConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OmDetTurboConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PatchTSMixerConfig, PatchTSTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PhiConfig, Phi3Config, PhimoeConfig, PixtralVisionConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, PvtV2Config, QDQBertConfig, Qwen2Config, Qwen2_5_VLConfig, Qwen2AudioEncoderConfig, Qwen2MoeConfig, Qwen2VLConfig, RecurrentGemmaConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RTDetrConfig, RTDetrV2Config, RwkvConfig, SamConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SegformerConfig, SegGptConfig, SEWConfig, SEWDConfig, SiglipConfig, SiglipVisionConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, StableLmConfig, Starcoder2Config, SuperGlueConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TextNetConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TimmWrapperConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, TvpConfig, UdopConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, UnivNetConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2BertConfig, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, ZambaConfig, Zamba2Config.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load SpatialRGPT model and processor\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# model_name = \"a8cheng/SpatialRGPT-VILA1.5-8B\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremyxai/SpaceLLaVA\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m ctxt_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA 3D model of a robot arm and a red dot with a green end-effector.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:567\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[0;32m--> 567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'> for this kind of AutoModel: AutoModel.\nModel type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, AriaConfig, AriaTextConfig, ASTConfig, AutoformerConfig, BambaConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChameleonConfig, ChineseCLIPConfig, ChineseCLIPVisionConfig, ClapConfig, CLIPConfig, CLIPTextConfig, CLIPVisionConfig, CLIPSegConfig, ClvpConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, DabDetrConfig, DacConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DbrxConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DepthProConfig, DetaConfig, DetrConfig, DiffLlamaConfig, DinatConfig, Dinov2Config, Dinov2WithRegistersConfig, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FalconMambaConfig, FastSpeech2ConformerConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GemmaConfig, Gemma2Config, GitConfig, GlmConfig, GLPNConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeSharedConfig, GraphormerConfig, GroundingDinoConfig, GroupViTConfig, HeliumConfig, HieraConfig, HubertConfig, IBertConfig, IdeficsConfig, Idefics2Config, Idefics3Config, Idefics3VisionConfig, IJepaConfig, ImageGPTConfig, InformerConfig, JambaConfig, JetMoeConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MambaConfig, Mamba2Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MimiConfig, MistralConfig, MixtralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, ModernBertConfig, MoonshineConfig, MoshiConfig, MPNetConfig, MptConfig, MraConfig, MT5Config, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NatConfig, NemotronConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OmDetTurboConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PatchTSMixerConfig, PatchTSTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PhiConfig, Phi3Config, PhimoeConfig, PixtralVisionConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, PvtV2Config, QDQBertConfig, Qwen2Config, Qwen2_5_VLConfig, Qwen2AudioEncoderConfig, Qwen2MoeConfig, Qwen2VLConfig, RecurrentGemmaConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RTDetrConfig, RTDetrV2Config, RwkvConfig, SamConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SegformerConfig, SegGptConfig, SEWConfig, SEWDConfig, SiglipConfig, SiglipVisionConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, StableLmConfig, Starcoder2Config, SuperGlueConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TextNetConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TimmWrapperConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, TvpConfig, UdopConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, UnivNetConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2BertConfig, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, ZambaConfig, Zamba2Config."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel,ChineseCLIPModel,ChineseCLIPProcessor\n",
    "\n",
    "# Load CLIP model and processor\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# model = ChineseCLIPModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n",
    "# processor = ChineseCLIPProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "# Load SpatialRGPT model and processor\n",
    "\n",
    "# model_name = \"a8cheng/SpatialRGPT-VILA1.5-8B\"\n",
    "# model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "# processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model_name = \"remyxai/SpaceLLaVA\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "ctxt_prompt = \"A 3D model of a robot arm and a red dot with a green end-effector.\"\n",
    "\n",
    "# Text descriptions\n",
    "text_descriptions = [\n",
    "    # ctxt_prompt + \"The robot arm's green end effector is touching the red dot\",\n",
    "    # ctxt_prompt + \"The robot arm's green end-effector is on the top part of the image\",\n",
    "    # ctxt_prompt + \"The robot arm's green end-effector is on the bottom part of the image\",\n",
    "    # ctxt_prompt + \"The robot arm's green end-effector is on the right part of the image\",\n",
    "    # ctxt_prompt + \"The robot arm's green end-effector is on the left part of the image\",\n",
    "    # \"A picture of a 3D humanoid kneeling down.\",\n",
    "    # \"3D robot arm\",\n",
    "    # \"3D model\",\n",
    "    # \"3D simulation\",\n",
    "    # \"an elephant jumping and down\",\n",
    "    ctxt_prompt + \"The robot arm's green end effector is touching the red dot\",\n",
    "    ctxt_prompt + \"The robot arm's green end effector is not at all touching the red dot\",\n",
    "    \"The robot arm's green end effector is touching the red dot\",\n",
    "    \"The robot arm's green end effector is not at all touching the red dot\",\n",
    "    ctxt_prompt + \"The robot arm is extending to the left side\",\n",
    "    ctxt_prompt + \"The robot arm is extending to the right side\",\n",
    "    ctxt_prompt + \"The robot arm is extending to the top side\",\n",
    "    ctxt_prompt + \"The robot arm is extending to the bottom side\",\n",
    "    \"The robot arm is extending to the left side\",\n",
    "    \"The robot arm is extending to the right side\",\n",
    "    \"The robot arm is extending to the top side\",\n",
    "    \"The robot arm is extending to the bottom side\",\n",
    "    \"The green circle is touching the red dot\",\n",
    "    \"The green circle is not at all touching the red dot\",\n",
    "]\n",
    "\n",
    "ch_ctxt_prompt = \"一个 3D 机器人手臂模型和一个红色点，其中机器人的绿色末端执行器。\"\n",
    "\n",
    "ch_text_descriptions = [\n",
    "    ch_ctxt_prompt + \" 机器人手臂的绿色末端执行器正在触碰红色点。\",\n",
    "    ch_ctxt_prompt + \" 机器人手臂的绿色末端执行器完全没有触碰红色点。\",\n",
    "    \"机器人手臂的绿色末端执行器正在触碰红色点。\",\n",
    "    \"机器人手臂的绿色末端执行器完全没有触碰红色点。\",\n",
    "    ch_ctxt_prompt + \" 机器人手臂正在向左侧伸展。\",\n",
    "    ch_ctxt_prompt + \" 机器人手臂正在向右侧伸展。\",\n",
    "    ch_ctxt_prompt + \" 机器人手臂正在向上方伸展。\",\n",
    "    ch_ctxt_prompt + \" 机器人手臂正在向下方伸展。\",\n",
    "    \"机器人手臂正在向左侧伸展。\",\n",
    "    \"机器人手臂正在向右侧伸展。\",\n",
    "    \"机器人手臂正在向上方伸展。\",\n",
    "    \"机器人手臂正在向下方伸展。\",\n",
    "    \"绿色圆形正在触碰红色点。\",\n",
    "    \"绿色圆形完全没有触碰红色点。\",\n",
    "]\n",
    "\n",
    "# Process inputs\n",
    "# inputs = processor(text=ch_text_descriptions, images=rx_img, return_tensors=\"pt\", padding=True)\n",
    "inputs = processor(text=text_descriptions, images=rx_img, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1) \n",
    "\n",
    "sorted_indices = torch.argsort(probs, descending=True)\n",
    "sorted_probs = probs[0][sorted_indices]\n",
    "sorted_desc = np.array(text_descriptions)[sorted_indices]\n",
    "\n",
    "for desc,prob in zip(sorted_desc[0],sorted_probs[0].detach().numpy()):\n",
    "    print(f\"{desc} : {prob:.2f}\")\n",
    "\n",
    "\n",
    "# sim_inputs = processor(text=[\"A 3D model of a robot arm and a red dot with a green end-effector. The robot arm's green end effector is touching the red dot\"], images=rx_img, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# # Compute text and image features\n",
    "# with torch.no_grad():\n",
    "#     text_features = model.get_text_features(**{k: sim_inputs[k] for k in [\"input_ids\", \"attention_mask\"]})\n",
    "#     image_features = model.get_image_features(**{k: sim_inputs[k] for k in [\"pixel_values\"]})\n",
    "\n",
    "# # Normalize features\n",
    "# text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "# image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# # Compute similarity score\n",
    "# similarity_score = (text_features @ image_features.T) \n",
    "\n",
    "# # Goal / Baseline text processing\n",
    "# goal_prompt = \"A 3D model of a robot arm and a red dot with a green end-effector. The robot arm's green end effector is not touching the red dot\"\n",
    "# baseline_prompt = \"A 3D model of a robot arm and a red dot with a green end-effector.\"\n",
    "\n",
    "# goal_text_input = processor(text=[goal_prompt], return_tensors=\"pt\", padding=True)\n",
    "# baseline_text_input = processor(text=[baseline_prompt], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     goal_text_features = model.get_text_features(**{k: goal_text_input[k] for k in [\"input_ids\", \"attention_mask\"]})\n",
    "#     baseline_text_features = model.get_text_features(**{k: baseline_text_input[k] for k in [\"input_ids\", \"attention_mask\"]})\n",
    "\n",
    "# # Normalize goal and baseline text features\n",
    "# goal_norm_text_features = goal_text_features / goal_text_features.norm(dim=-1, keepdim=True)\n",
    "# baseline_norm_text_features = baseline_text_features / baseline_text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# goal_baseline_line = goal_norm_text_features - baseline_norm_text_features\n",
    "# goal_baseline_norm_line = goal_baseline_line / goal_baseline_line.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# # Projection of image features onto the goal-baseline line\n",
    "# proj_img = (goal_baseline_norm_line @ image_features.T) * goal_baseline_norm_line\n",
    "\n",
    "# # Compute CLIP-Reg reward\n",
    "# clip_reg_alpha = 0.5\n",
    "# clip_reg = 1 - 0.5 * torch.sum(((clip_reg_alpha * proj_img + (1 - clip_reg_alpha) * image_features - goal_baseline_norm_line)) ** 2, dim=-1)\n",
    "\n",
    "# # Print results\n",
    "# print(\"Similarity Score:\")\n",
    "# print(similarity_score)\n",
    "# print(\"\\nLogits Per Image:\")\n",
    "# print(logits_per_image)\n",
    "# print(\"\\nProbs Per Image:\")\n",
    "# print(probs)\n",
    "# print(\"\\nCLIP-Reg:\")\n",
    "# print(clip_reg)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
